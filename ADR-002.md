Of course. This is an excellent evolution of the original idea, as it retains the key benefits of the declarative approach while significantly increasing flexibility and lowering the barrier to entry.

Here is an updated Architecture Decision Record that reflects this decoupled approach.

---

# ADR-002: Declarative AI System Definitions using Kubernetes-style Schemas with Runtime Independence

**Status:** Accepted

**Date:** 2025-09-01

**Supersedes:** [ADR-001: Declarative, Kubernetes-Native Approach for AI System Definitions](link-to-adr-001)

## Context

Our initial analysis (ADR-001) identified significant challenges with imperative, code-first approaches to building AI systems, including a lack of standardization, poor governance, and operational complexity. We decided that a declarative "configuration-as-code" model was the correct path forward.

The initial proposal was to tightly couple this declarative model with the Kubernetes ecosystem by implementing it as Custom Resource Definitions (CRDs) managed by Kubernetes operators. However, further consideration revealed potential drawbacks to this tight coupling:

*   **Runtime Rigidity:** It would force all AI development onto Kubernetes, excluding other viable and often simpler runtimes like serverless functions (e.g., AWS Lambda, Google Cloud Functions), edge computing environments, or even local developer machines.
*   **High Barrier to Entry:** It would require all teams involved in AI development to have a deep understanding of Kubernetes concepts, which is a significant operational and educational burden.
*   **Operational Bottleneck:** A centralized, operator-based runtime could become a bottleneck and a single point of failure if not managed with extreme care.

The core value of our proposal is the **declarative specification itself**, not the specific runtime that executes it. The challenge is to retain the benefits of the declarative model (standardization, GitOps, governance) while allowing for flexible, polyglot runtime implementations.

## Decision

We will adopt a **declarative-first approach using Kubernetes-style YAML schemas** as the canonical source of truth for defining our AI systems.

Crucially, this decision **decouples the specification schema from the runtime implementation**. While the schema is inspired by and compatible with Kubernetes CRDs, the runtime is not required to be a Kubernetes-native operator.

The canonical definitions for our AI systems will remain the same set of schemas:
1.  **`Agent`**: Defines a single AI agent (its persona, model, tools, etc.).
2.  **`KnowledgeBase`**: Defines a RAG data source and its ingestion/retrieval pipeline.
3.  **`AgentWorkflow`**: Defines a multi-agent orchestration.

These YAML specifications will be the "intent." The "execution" can be achieved through two primary, non-exclusive pathways:

1.  **Runtime Interpretation:** A generic runtime environment (e.g., a serverless function, a containerized application) is designed to bootstrap itself by reading a given specification YAML at startup. Frameworks like Agno or Mastra could implement this pattern, where the same runtime binary can behave as any agent simply by being provided a different spec file.

2.  **Build-Time Transpilation (Code Generation):** A CI/CD pipeline step takes a specification YAML as input and transpiles it into framework-specific source code or configuration. For example, an `Agent` spec could be converted into Python code that utilizes the LangChain or LlamaIndex SDKs, with all prompts, model parameters, and tool bindings pre-configured.

This approach allows teams to choose the best runtime for their specific use case, skill set, and operational maturity, while still adhering to a single, unified standard for defining what their AI system *is*.

## Consequences

### Positive
*   **Runtime Flexibility:** This is the primary advantage. Teams can deploy AI systems to Kubernetes, serverless platforms, edge devices, or even run them locally for development, all from the same declarative source of truth. This avoids technology lock-in.
*   **Configuration-as-Code and GitOps:** This core benefit is fully retained. The YAML specification is the auditable, version-controlled contract.
*   **Standardization and Governance:** The schemas provide a common language and a clear contract for AI systems across the organization, enabling consistent security and governance checks regardless of the deployment target.
*   **Reduced Barrier to Entry:** Teams can start building and deploying agents in a simple serverless environment without needing to become Kubernetes experts. This drastically lowers the operational overhead and learning curve.
*   **Improved Developer Experience:** A developer can write an `Agent` spec and test it immediately using a lightweight local runtime that interprets the YAML, ensuring high fidelity between development and production.
*   **Stronger Decoupling of Concerns:** The separation is even cleaner. Platform teams can offer a menu of certified runtimes (a "gold-plated" Kubernetes operator, a cost-effective serverless runtime, a high-performance Go binary) that all conform to the same input specification.

### Negative
*   **Engineering Effort Shifted to Runtimes:** Instead of building one complex Kubernetes operator, we must now build and maintain multiple runtimes and/or transpilers. Ensuring these different runtimes have feature parity is a significant, ongoing engineering challenge.
*   **Risk of Implementation Inconsistency:** Different runtimes might interpret ambiguous parts of the specification differently, leading to subtle behavioral changes. A robust conformance testing suite becomes critical to certify that a given runtime fully adheres to the spec.
*   **Loss of Native Kubernetes Lifecycle Management:** When not using the operator model, we lose the powerful, built-in capabilities of the Kubernetes control plane for our AI assets. Self-healing, scaling (HPA), rolling updates, and service discovery must be managed at the level of the chosen runtime (e.g., via serverless configuration, manual scaling rules) rather than being an intrinsic property of the `Agent` resource itself.
*   **Increased Schema Governance Overhead:** Since the schema is the central contract for multiple runtimes, changes to it must be managed with extreme care. A breaking change to the `Agent` spec could require coordinated updates across several runtime projects.

## Alternatives Considered

### 1. The Pure Kubernetes Operator Model (ADR-001)
*   **Description:** All specifications are implemented as true Kubernetes CRDs, managed exclusively by operators.
*   **Pros:** Powerful, self-healing, inherits the full power of the Kubernetes ecosystem.
*   **Cons (Why Rejected for Now):** As outlined in the context, this approach was deemed too rigid and imposed too high an operational cost on all teams. This new decision (ADR-002) allows the operator model to exist as *one of the possible runtimes*, not the only one.

### 2. Imperative SDKs / Libraries
*   **Description:** Provide developers with libraries to define agents in code.
*   **Cons (Why Rejected):** This remains rejected for the same reasons as in ADR-001. It fails to provide standardization, governance, and a declarative, auditable definition of the AI system.

### 3. Centralized UI-Driven / Database-Backed Platform
*   **Description:** A web-based platform for building agents.
*   **Cons (Why Rejected):** This remains rejected as it is fundamentally incompatible with our "as-code" and GitOps principles, hindering versioning, auditing, and automated CI/CD.

This updated decision provides a more flexible and pragmatic path forward, capturing the benefits of a declarative approach while empowering teams with the freedom to choose the right runtime for their needs.
